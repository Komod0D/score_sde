{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/komodo/miniconda3/envs/thesis/lib/python3.6/site-packages/flax/nn/__init__.py:35: DeprecationWarning: The `flax.nn` module is Deprecated, use `flax.linen` instead. Learn more and find an upgrade guide at https://github.com/google/flax/blob/master/flax/linen/README.md\n",
      "  warnings.warn(\"The `flax.nn` module is Deprecated, use `flax.linen` instead. Learn more and find an upgrade guide at https://github.com/google/flax/blob/master/flax/linen/README.md\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from models import utils as mutils\n",
    "import jax\n",
    "from jax import numpy as jnp\n",
    "from configs.ve.disk_ssim import get_config\n",
    "from models import super_simple\n",
    "\n",
    "import losses\n",
    "import sde_lib\n",
    "\n",
    "import functools\n",
    "import sampling\n",
    "import datasets\n",
    "from flax import jax_utils as flax_utils\n",
    "from models import layers, layerspp\n",
    "\n",
    "from flax import linen as nn\n",
    "from matplotlib import pyplot as plt\n",
    "from flax.training import checkpoints\n",
    "\n",
    "default_init = layers.default_init\n",
    "get_act = layers.get_act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_step_fn(sde, model, train, optimize_fn=None, reduce_mean=False, continuous=True, likelihood_weighting=False):\n",
    "  \"\"\"Create a one-step training/evaluation function.\n",
    "\n",
    "  Args:\n",
    "    sde: An `sde_lib.SDE` object that represents the forward SDE.\n",
    "    model: A `flax.linen.Module` object that represents the architecture of the score-based model.\n",
    "    train: `True` for training and `False` for evaluation.\n",
    "    optimize_fn: An optimization function.\n",
    "    reduce_mean: If `True`, average the loss across data dimensions. Otherwise sum the loss across data dimensions.\n",
    "    continuous: `True` indicates that the model is defined to take continuous time steps.\n",
    "    likelihood_weighting: If `True`, weight the mixture of score matching losses according to\n",
    "      https://arxiv.org/abs/2101.09258; otherwise use the weighting recommended by our paper.\n",
    "\n",
    "  Returns:\n",
    "    A one-step function for training or evaluation.\n",
    "  \"\"\"\n",
    "  if continuous:\n",
    "    loss_fn = losses.get_sde_loss_fn(sde, model, train, reduce_mean=reduce_mean,\n",
    "                              continuous=True, likelihood_weighting=likelihood_weighting)\n",
    "  else:\n",
    "    raise NotImplementedError()\n",
    "\n",
    "  def step_fn(carry_state, batch):\n",
    "    \"\"\"Running one step of training or evaluation.\n",
    "\n",
    "    This function will undergo `jax.lax.scan` so that multiple steps can be pmapped and jit-compiled together\n",
    "    for faster execution.\n",
    "\n",
    "    Args:\n",
    "      carry_state: A tuple (JAX random state, `flax.struct.dataclass` containing the training state).\n",
    "      batch: A mini-batch of training/evaluation data.\n",
    "\n",
    "    Returns:\n",
    "      new_carry_state: The updated tuple of `carry_state`.\n",
    "      loss: The average loss value of this state.\n",
    "    \"\"\"\n",
    "\n",
    "    (rng, state) = carry_state\n",
    "    rng, step_rng = jax.random.split(rng)\n",
    "    grad_fn = jax.value_and_grad(loss_fn, argnums=1, has_aux=True)\n",
    "    if train:\n",
    "      params = state.optimizer.target\n",
    "      states = state.model_state\n",
    "      (loss, new_model_state), grad = grad_fn(step_rng, params, states, batch)\n",
    "      new_optimizer = optimize_fn(state, grad)\n",
    "      new_params_ema = jax.tree_multimap(\n",
    "        lambda p_ema, p: p_ema * state.ema_rate + p * (1. - state.ema_rate),\n",
    "        state.params_ema, new_optimizer.target\n",
    "      )\n",
    "      step = state.step + 1\n",
    "      new_state = state.replace(\n",
    "        step=step,\n",
    "        optimizer=new_optimizer,\n",
    "        model_state=new_model_state,\n",
    "        params_ema=new_params_ema\n",
    "      )\n",
    "    else:\n",
    "      loss, _ = loss_fn(step_rng, state.params_ema, state.model_state, batch)\n",
    "      new_state = state\n",
    "\n",
    "    new_carry_state = (rng, new_state)\n",
    "    return new_carry_state, loss\n",
    "\n",
    "  return step_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec = {'image': tf.TensorSpec(shape=(1, 1, 2), dtype=tf.float32, name=None),\n",
    "            'label': tf.TensorSpec(shape=(), dtype=tf.int32, name=None)}\n",
    "new_ds = tf.data.experimental.load('disk/data', spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "step_rng = jax.random.PRNGKey(42)\n",
    "config = get_config()\n",
    "\n",
    "train_ds = new_ds\n",
    "train_iter = iter(train_ds)\n",
    "temp = next(train_iter)\n",
    "ts = jax.random.uniform(jax.random.PRNGKey(42), shape=(1,)) * 999\n",
    "x = temp['image']\n",
    "rng = jax.random.PRNGKey(24)\n",
    "labels = ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 1, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_model = super_simple.SSimple(config=config)\n",
    "variables = score_model.init({'params': rng, 'dropout': rng}, x, ts)\n",
    "# Variables is a `flax.FrozenDict`. It is immutable and respects functional programming\n",
    "init_model_state, initial_params = variables.pop('params')\n",
    "x, params = score_model.apply(variables, x, labels, train=True, mutable=list(init_model_state.keys()), rngs={'dropout': rng})\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, eval_ds, _ = datasets.get_dataset(config,\n",
    "                                              additional_dim=config.training.n_jitted_steps,\n",
    "                                              uniform_dequantization=config.data.uniform_dequantization)\n",
    "\n",
    "train_iter = iter(train_ds)\n",
    "temp = next(train_iter)\n",
    "batch = jax.tree_map(lambda x: x._numpy(), temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 1, 1, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = batch['image'][0, 0]\n",
    "t = jax.random.uniform(rng, (64,))\n",
    "out, params = score_model.apply(variables, x, t, train=True, mutable=list(init_model_state.keys()), rngs={'dropout': rng})\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = jax.random.PRNGKey(41)\n",
    "optimizer = losses.get_optimizer(config).create(initial_params)\n",
    "state = mutils.State(step=0, optimizer=optimizer, lr=config.optim.lr,\n",
    "                       model_state=init_model_state,\n",
    "                       ema_rate=config.model.ema_rate,\n",
    "                       params_ema=initial_params,\n",
    "                       rng=rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = state.optimizer.target\n",
    "states = state.model_state\n",
    "sde = sde_lib.VESDE(sigma_min=config.model.sigma_min, sigma_max=config.model.sigma_max, N=config.model.num_scales)\n",
    "sampling_eps = 1e-5\n",
    "\n",
    "optimize_fn = losses.optimization_manager(config)\n",
    "continuous = config.training.continuous\n",
    "reduce_mean = config.training.reduce_mean\n",
    "likelihood_weighting = config.training.likelihood_weighting\n",
    "\n",
    "\n",
    "n_jitted_steps = config.training.n_jitted_steps\n",
    "# Must be divisible by the number of steps jitted together\n",
    "assert config.training.log_freq % n_jitted_steps == 0 and \\\n",
    "        config.training.snapshot_freq_for_preemption % n_jitted_steps == 0 and \\\n",
    "        config.training.eval_freq % n_jitted_steps == 0 and \\\n",
    "        config.training.snapshot_freq % n_jitted_steps == 0, \"Missing logs or checkpoints!\"\n",
    "train_ds, eval_ds, _ = datasets.get_dataset(config,\n",
    "                                              additional_dim=config.training.n_jitted_steps,\n",
    "                                              uniform_dequantization=config.data.uniform_dequantization)\n",
    "\n",
    "train_iter = iter(train_ds)\n",
    "temp = next(train_iter)\n",
    "(temp['image'][0, 0, 0], temp['label'][0, 0, 0])\n",
    "batch = jax.tree_map(lambda x: x._numpy(), temp)\n",
    "\n",
    "loss_fn = losses.get_sde_loss_fn(sde, score_model, True, reduce_mean=reduce_mean,\n",
    "                              continuous=True, likelihood_weighting=likelihood_weighting)\n",
    "\n",
    "grad_fn = jax.value_and_grad(loss_fn, argnums=1, has_aux=True)\n",
    "\n",
    "(loss, new_model_state), grad = grad_fn(step_rng, params, states, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step_fn = get_step_fn(sde, score_model, train=True, optimize_fn=optimize_fn,\n",
    "                                    reduce_mean=reduce_mean, continuous=continuous,\n",
    "                                    likelihood_weighting=likelihood_weighting)\n",
    "a, b = train_step_fn((rng, state), batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "if config.training.sde.lower() == 'vpsde':\n",
    "    sde = sde_lib.VPSDE(beta_min=config.model.beta_min, beta_max=config.model.beta_max, N=config.model.num_scales)\n",
    "    sampling_eps = 1e-3\n",
    "elif config.training.sde.lower() == 'subvpsde':\n",
    "    sde = sde_lib.subVPSDE(beta_min=config.model.beta_min, beta_max=config.model.beta_max, N=config.model.num_scales)\n",
    "    sampling_eps = 1e-3\n",
    "elif config.training.sde.lower() == 'vesde':\n",
    "    sde = sde_lib.VESDE(sigma_min=config.model.sigma_min, sigma_max=config.model.sigma_max, N=config.model.num_scales)\n",
    "    sampling_eps = 1e-5\n",
    "else:\n",
    "    raise NotImplementedError(f\"SDE {config.training.sde} unknown.\")\n",
    "inverse_scaler = datasets.get_data_inverse_scaler(config)\n",
    "\n",
    "\n",
    "# Build one-step training and evaluation functions\n",
    "optimize_fn = losses.optimization_manager(config)\n",
    "continuous = config.training.continuous\n",
    "reduce_mean = config.training.reduce_mean\n",
    "likelihood_weighting = config.training.likelihood_weighting\n",
    "train_step_fn = losses.get_step_fn(sde, score_model, train=True, optimize_fn=optimize_fn,\n",
    "                                    reduce_mean=reduce_mean, continuous=continuous,\n",
    "                                    likelihood_weighting=likelihood_weighting)\n",
    "# Pmap (and jit-compile) multiple training steps together for faster running\n",
    "p_train_step = jax.pmap(functools.partial(jax.lax.scan, train_step_fn), axis_name='batch', donate_argnums=1)\n",
    "eval_step_fn = losses.get_step_fn(sde, score_model, train=False, optimize_fn=optimize_fn,\n",
    "                                reduce_mean=reduce_mean, continuous=continuous,\n",
    "                                likelihood_weighting=likelihood_weighting)\n",
    "# Pmap (and jit-compile) multiple evaluation steps together for faster running\n",
    "p_eval_step = jax.pmap(functools.partial(jax.lax.scan, eval_step_fn), axis_name='batch', donate_argnums=1)\n",
    "\n",
    "# Building sampling functions\n",
    "if config.training.snapshot_sampling:\n",
    "    sampling_shape = (config.training.batch_size // jax.local_device_count(), config.data.image_size,\n",
    "                    config.data.image_size, config.data.num_channels)\n",
    "sampling_fn = sampling.get_sampling_fn(config, sde, score_model, sampling_shape, inverse_scaler, sampling_eps)\n",
    "\n",
    "# Replicate the training state to run on multiple devices\n",
    "pstate = flax_utils.replicate(state)\n",
    "num_train_steps = config.training.n_iters\n",
    "\n",
    "# In case there are multiple hosts (e.g., TPU pods), only log to host 0\n",
    "rng = jax.random.fold_in(rng, jax.host_id())\n",
    "\n",
    "# JIT multiple training steps together for faster training\n",
    "n_jitted_steps = config.training.n_jitted_steps\n",
    "# Must be divisible by the number of steps jitted together\n",
    "assert config.training.log_freq % n_jitted_steps == 0 and \\\n",
    "        config.training.snapshot_freq_for_preemption % n_jitted_steps == 0 and \\\n",
    "        config.training.eval_freq % n_jitted_steps == 0 and \\\n",
    "        config.training.snapshot_freq % n_jitted_steps == 0, \"Missing logs or checkpoints!\"\n",
    "train_ds, eval_ds, _ = datasets.get_dataset(config,\n",
    "                                              additional_dim=config.training.n_jitted_steps,\n",
    "                                              uniform_dequantization=config.data.uniform_dequantization)\n",
    "config.training.batch_size\n",
    "train_iter = iter(train_ds)\n",
    "temp = next(train_iter)\n",
    "(temp['image'][0, 0, 0], temp['label'][0, 0, 0])\n",
    "batch = jax.tree_map(lambda x: x._numpy(), temp)\n",
    "rng, *next_rng = jax.random.split(rng, num=jax.local_device_count() + 1)\n",
    "next_rng = jnp.asarray(next_rng)\n",
    "# Execute one training step\n",
    "(_, pstate), ploss = p_train_step((next_rng, pstate), batch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
